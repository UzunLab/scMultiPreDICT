# scMultiPreDICT: A single-cell predictive framework with transcriptomic and epigenetic signatures

## Table of Contents

1. [Overview](#overview)
2. [Pipeline Steps](#pipeline-steps)
3. [Prerequisites](#prerequisites)
4. [Quick Start (Single Dataset)](#quick-start-single-dataset)
5. [Running Multiple Datasets](#running-multiple-datasets)
6. [Repository Structure](#repository-structure)
7. [High-Performance Computing (HPC) Usage](#high-performance-computing-hpc-usage)
8. [Target Gene Configuration](#target-gene-configuration)
9. [Reproducing Published Results](#reproducing-published-results)
10. [Data Availability](#data-availability)
11. [Citation](#citation)
12. [License](#license)
13. [Troubleshooting](#troubleshooting)
14. [Contact](#contact)

## Overview

**scMultiPreDICT** (single-cell Multimodal Predictor of Gene Expression via Integrated Chromatin and Transcriptome) is a computational framework for predicting gene expression levels from single-cell multiome (joint RNA-seq and ATAC-seq) data. 

This pipeline takes paired scRNA-seq and scATAC-seq data (multiome) and trains models to predict gene expression from:
- **ATAC peaks** within a window around each gene's TSS
- **Highly Variable Gene (HVG) expression** from other genes

**Framework Overview**

![alt text](Workflow_Framework_Figure1.png)

### What the Pipeline Does
1. **Quality Control**: Filter cells and genes based on QC metrics
2. **Data Splitting**: Stratified train/validation/test splits
3. **Metacell Creation**: k-NN smoothing to reduce noise while preserving cell identity
4. **Feature Extraction**: Extract ATAC peaks and HVG features per target gene
5. **Model Training**: Train Linear, Elastic Net, Random Forest, and Neural Network models

### Key Predictors

- **Three Feature Sets**
  - **RNA-only**: RNA expression features only (top HVGs)
  - **ATAC-only**: Chromatin accessibility features only (peaks within ±250kb from transcription start site, specific to each target gene)
  - **Combined**: ATAC peaks (±250kb from transcription start site) + highly variable gene (HVG) RNA expression features


- **Dimensionality Reduction and Integrated Analysis Approaches for Metacell Construction**
  - **Combined pipeline**: PCA+LSI, Weighted Nearest Neighbor (WNN), scVI+PeakVI, MultiVI
  - **RNA-only pipeline**: PCA, scVI
  - **ATAC-only pipeline**: LSI, PeakVI

- **Comprehensive Model Comparison**
  - Ordinary Least Squares (OLS)
  - Ridge Regression (L2 regularization)
  - Lasso Regression (L1 regularization)
  - Elastic Net (combined L1/L2 regularization)
  - Random Forest
  - Deep Neural Networks (DeepNN)


## Pipeline Steps

### Combined Pipeline (Steps 1-6)

| Step | Script | Description |
|------|--------|-------------|
| 1 | `01_quality_control.R` | Cell filtering based on RNA and ATAC quality metrics |
| 2 | `02a_data_splitting.R` | Stratified splitting into training, validation, and test sets (70/20/10) |
| 3 | `03a_metacell_creation.R` | Metacell construction via k-nearest neighbor smoothing |
| 3b | `03b_export_to_mudata.R` | Export to MuData format for Python-based autoencoder training |
| 4 | `04_feature_extraction.R` | Extraction of gene-specific RNA and ATAC features |
| 5 | `05_linear_tree_models.R` | Training and prediction with linear models and Random Forest |
| 6 | `06_neural_network.R` | Training and prediction with deep neural networks |
| _optional_ | `02b_select_target_genes.R` | Auto-select target genes for new datasets (standalone script, not part of default pipeline) |

> **Note:** The pipeline uses **pre-computed target gene lists** from `data/target_genes/`. Script `02b_select_target_genes.R` is only needed when analyzing entirely new datasets without existing gene lists.

### Single-Modality Pipelines

RNA-only and ATAC-only pipelines skip QC and data splitting, starting directly at metacell creation. They require the QC-filtered Seurat object and data splits generated by the combined pipeline (Steps 1-2).


## Prerequisites

### 1. Clone the Repository

```bash
git clone https://github.com/UzunLab/scMultiPreDICT.git
cd scMultiPreDICT
```

### 2. Install R Dependencies

```bash
# Install with mouse annotation packages
Rscript combined/install_packages.R --species=mouse

# Install with human annotation packages
Rscript combined/install_packages.R --species=human

# Install core packages only (add species packages later)
Rscript combined/install_packages.R
```

Or run interactively in R:

```r
# CRAN packages
install.packages(c(
  # Core
  "Seurat", "Matrix", "dplyr", "tidyr", "ggplot2",
  # Machine learning
  "glmnet", "ranger", "caret",
  # Utilities
  "RANN", "reticulate", "patchwork", "viridis"
))

# Bioconductor packages
BiocManager::install(c(
  "Signac", "GenomicRanges", "GenomeInfoDb", 
  "rtracklayer", "IRanges", "S4Vectors"
))

# Species-specific (install one based on your data)
BiocManager::install("EnsDb.Mmusculus.v79")     # Mouse
BiocManager::install("EnsDb.Hsapiens.v86")      # Human
```

### 3. Install Python Dependencies (for neural networks and autoencoder methods)

```bash
pip install -r requirements.txt
```
> **Note:** Neural network training uses TensorFlow/Keras via Python (through the `reticulate` R package), not the R keras package.

### 4. Verify Installation

```bash
Rscript test_installation.R
```

This checks all required R and Python dependencies and reports any missing packages.


## Quick Start (Single Dataset)

### Selecting the Appropriate Pipeline

| Pipeline | Features Used | Recommended Use Case |
|----------|--------------|----------------------|
| `combined/` | RNA + ATAC | Full multiome analysis|
| `rna_only/` | RNA only | Evaluating gene expression-only prediction capability |
| `atac_only/` | ATAC only | Evaluating chromatin accessibility-only prediction capability |

### Running the Combined Pipeline

#### 1. Input Data Requirements

You need the following files:

1. **RNA count matrix** (Matrix Market format):
   - `matrix.mtx.gz` - Sparse count matrix
   - `features.tsv.gz` - Gene names
   - `barcodes.tsv.gz` - Cell barcodes

2. **ATAC fragments file**:
   - `fragments.tsv.gz` - Sorted and indexed
   - `fragments.tsv.gz.tbi` - Tabix index

#### 2. Set Key Parameters in config.R
- Copy and Edit Configuration
```bash
cd combined
cp R/config_template.R config.R
# Edit config.R with YOUR dataset-specific paths and parameters
nano config.R
# Key settings to modify:
#   - SAMPLE_NAME: Your sample identifier
#   - INPUT_MTX, INPUT_FEATURES, INPUT_BARCODES: Paths to your 10X files
#   - INPUT_FRAGMENTS: Path to your ATAC fragments file
#   - SPECIES: "mouse" or "human"
#   - BASE_OUTPUT_DIR: Where to save outputs
```

```r
# Sample name
SAMPLE_NAME <- "my_sample"

# Input paths
INPUT_MTX <- "/path/to/matrix.mtx.gz"
INPUT_FEATURES <- "/path/to/features.tsv.gz"
INPUT_BARCODES <- "/path/to/barcodes.tsv.gz"
INPUT_FRAGMENTS <- "/path/to/fragments.tsv.gz"

# Output directory
BASE_OUTPUT_DIR <- "/path/to/output"

# Species: "mouse" or "human"
SPECIES <- "mouse"

# Conda environment with TensorFlow/Keras (used by Step 6 - Neural Networks)
CONDA_ENV_NAME <- "your_conda_env"
```

### Dimensionality Reduction Methods

| Pipeline | Method | Description |
|----------|--------|-------------|
| `combined/` | `pca_lsi` | PCA for RNA + LSI for ATAC, concatenated embeddings |
| | `wnn` | Weighted Nearest Neighbor integration (Seurat v4+) |
| | `scvi_peakvi` | Separate scVI and PeakVI autoencoders |
| | `multivi` | Joint MultiVI autoencoder |
| `rna_only/` | `pca` | Principal Component Analysis |
| `atac_only/` | `lsi` | Latent Semantic Indexing |

Configure the method in your `config.R`:

```r
DIM_REDUCTION_METHOD <- "pca_lsi"  # Options: "pca_lsi", "wnn", "scvi_peakvi", "multivi"
```

#### 3. Run the Pipeline

**Option A: Run all steps for one datatset**

```bash
cd combined
Rscript run_pipeline.R --config config.R

# Or run specific steps only
Rscript run_pipeline.R --config config.R --steps 1,2,3
Rscript run_pipeline.R --config config.R --start 3 --stop 5
```

**Option B: Multi-dataset submission** — see [Running Multiple Datasets](#running-multiple-datasets) below.

### Running Single-Modality Pipelines

Single-modality pipelines (RNA-only, ATAC-only) require preprocessing outputs from the combined pipeline:

```bash
# Step 1: Run quality control and data splitting from combined pipeline
cd combined
Rscript run_pipeline.R --config config.R --steps 1,2

# Step 2: Run the single-modality pipeline
cd ../rna_only
cp config_template.R config.R
# Edit config.R to point to combined pipeline outputs
Rscript run_pipeline.R --config config.R
```

## Running Multiple Datasets

The multi-dataset submission system lets you run the pipeline across **many datasets** with a single command. Within each dataset, steps run **serially** (each step waits for the previous to finish). Across datasets, pipelines run **in parallel** (independent SLURM jobs).

```
Dataset A:  [Step1] → [Step2] → [Step3] → [Step4]     (serial)
Dataset B:  [Step1] → [Step2] → [Step3] → [Step4]     (serial)
Dataset C:  [Step1] → [Step2] → [Step3] → [Step4]     (serial)
            ↑ all three run simultaneously on the cluster (parallel)
```

This uses SLURM's `--dependency=afterok:<job_id>` flag to enforce step ordering within each dataset, while submitting all datasets independently.

### Setup (3 steps per pipeline)

#### Step 1: Configure Cluster Settings

```bash
cd combined/   # or rna_only/ or atac_only/
cp submit_settings.template.sh submit_settings.sh
nano submit_settings.sh
```

Edit the following in `submit_settings.sh`:
- `PARTITION` — your SLURM partition (e.g., `"compute"`)
- `CONDA_ENV_R` — conda environment with R and Bioconductor
- `LOG_BASE_DIR` — where SLURM log files go
- Per-step resources (`STEP_N_CPUS`, `STEP_N_MEM`, `STEP_N_TIME`)

#### Step 2: Create Per-Dataset Config Files

```bash
# Combined pipeline:
cp R/config_template.R datasets/E7.5_rep1.config.R
cp R/config_template.R datasets/E7.5_rep2.config.R
cp R/config_template.R datasets/T_Cells.config.R

# RNA-only or ATAC-only pipeline:
cp config_template.R datasets/E7.5_rep1.config.R
cp config_template.R datasets/T_Cells.config.R
```

Edit each `.config.R` file with **dataset-specific** values:
- `SAMPLE_NAME` — unique identifier for this dataset
- Input file paths (matrix, features, barcodes, fragments)
- `SPECIES` — `"mouse"` or `"human"`
- `BASE_OUTPUT_DIR` — output directory
- Target gene files (HVG/random gene lists)

#### Step 3: Submit

```bash
chmod +x submit_datasets.sh

# Preview what will happen (no jobs submitted)
./submit_datasets.sh --dry-run

# Submit all datasets
./submit_datasets.sh
```

### Multi-Dataset Command Options

```bash
# Submit all datasets, all steps
./submit_datasets.sh

# Dry run — preview job plan without submitting
./submit_datasets.sh --dry-run

# Submit only specific datasets
./submit_datasets.sh --datasets E7.5_rep1,T_Cells

# Start from a specific step (e.g., steps 1-2 already completed)
./submit_datasets.sh --start-step 3

# Run only a range of steps
./submit_datasets.sh --start-step 3 --stop-step 4

# Combine options
./submit_datasets.sh --datasets E7.5_rep1 --start-step 3 --dry-run
```

### Monitoring & Recovery

```bash
# Check all your running/pending jobs
squeue -u $USER

# View logs for a specific dataset
tail -f ~/scMultiPreDICT_logs/combined/E7.5_rep1/metacell_12345.log

# If a step fails, fix the issue and resubmit from that step
./submit_datasets.sh --datasets E7.5_rep1 --start-step 3

# Cancel all your jobs
scancel -u $USER
```

> **Note:** When a job fails, its downstream dependent jobs stay pending. Cancel them with `scancel`, fix the issue, and resubmit from the failed step.

For a detailed explanation of how serial/parallel SLURM execution works, see [`docs/SLURM_SERIAL_PARALLEL_EXPLAINED.md`](docs/SLURM_SERIAL_PARALLEL_EXPLAINED.md).

### Cross-Pipeline Orchestration (submit_all.sh)

The RNA-only and ATAC-only pipelines depend on the **combined pipeline's step 2** output (Seurat splits). The `submit_all.sh` script at the repo root handles this automatically:

```
COMBINED:  step1 → step2 → step3 → step4 → step5 → step6
                      ↓ (after step 2 completes)
RNA_ONLY:           rna1 → rna2 → rna3 → rna4
                      ↓ (after step 2 completes)
ATAC_ONLY:         atac1 → atac2 → atac3 → atac4
```

After combined step 2 finishes, three branches run **in parallel**: combined steps 3-6, RNA-only steps 1-4, and ATAC-only steps 1-4.

**Setup:** Create `submit_settings.sh` and dataset configs for each pipeline you want to run, then:

```bash
# Preview the full dependency graph
./submit_all.sh --dry-run

# Submit all pipelines for all datasets
./submit_all.sh

# Only combined + RNA (skip ATAC)
./submit_all.sh --pipelines combined,rna

# Specific datasets only
./submit_all.sh --datasets T_Cells,E7.5_rep1
```

> **Note:** You can still run individual pipelines with `cd combined/ && ./submit_datasets.sh` if you don't need cross-pipeline dependencies.

## Repository Structure

```
scMultiPreDICT/
├── submit_all.sh                      # Master orchestrator (combined → rna + atac)
├── combined/                          # Full RNA+ATAC pipeline (Steps 1-6)
│   ├── run_pipeline.R                 # Pipeline orchestration script
│   ├── install_packages.R             # R dependency installer
│   ├── config.R                       # Your config file (git-ignored, create from template)
│   ├── submit_datasets.sh             # Multi-dataset SLURM orchestrator
│   ├── submit_settings.template.sh    # Cluster settings template
│   ├── datasets/                      # Per-dataset config files
│   │   ├── README.md                  # Setup instructions
│   │   ├── E7.5_rep1.config.R         # (user-created)
│   │   └── T_Cells.config.R           # (user-created)
│   ├── slurm/
│   │   ├── run_step.sbatch            # Generic parameterized step runner
│   │   └── run_step_python.sbatch     # Python step runner (autoencoders)
│   ├── R/
│   │   ├── config_template.R                 # Configuration template (copy to ../config.R)
│   │   ├── 01_quality_control.R
│   │   ├── 02a_data_splitting.R
│   │   ├── 02b_select_target_genes.R         # Optional: auto-select genes for new datasets
│   │   ├── 03a_metacell_creation_pca_lsi.R   # PCA+LSI method (default)
│   │   ├── 03a_metacell_creation_wnn.R       # WNN method
│   │   ├── 03a_metacell_creation_scvi_peakvi.R
│   │   ├── 03a_metacell_creation_multivi.R
│   │   ├── 03b_export_to_mudata.R
│   │   ├── 04_feature_extraction.R
│   │   ├── 05_linear_tree_models.R
│   │   └── 06_neural_network.R
│   └── python/
│       └── train_autoencoders.py
│
├── rna_only/                          # RNA-only pipeline (Steps 1-4)
│   ├── config_template.R              # Configuration template
│   ├── config.R                       # Your config file (git-ignored)
│   ├── run_pipeline.R
│   ├── submit_datasets.sh             # Multi-dataset SLURM orchestrator
│   ├── submit_settings.template.sh    # Cluster settings template
│   ├── datasets/                      # Per-dataset config files
│   ├── slurm/
│   │   └── run_step.sbatch            # Generic parameterized step runner
│   └── R/
│
├── atac_only/                         # ATAC-only pipeline (Steps 1-4)
│   ├── config_template.R              # Configuration template
│   ├── config.R                       # Your config file (git-ignored)
│   ├── run_pipeline.R
│   ├── submit_datasets.sh             # Multi-dataset SLURM orchestrator
│   ├── submit_settings.template.sh    # Cluster settings template
│   ├── datasets/                      # Per-dataset config files
│   ├── slurm/
│   │   └── run_step.sbatch            # Generic parameterized step runner
│   └── R/
│
├── data/target_genes/                 # Pre-computed target gene lists
│   ├── E7.5_rep1/
│   ├── E7.5_rep2/
│   └── T_Cells/
│
├── models/                            # Pre-trained models for reproducibility
│   ├── README.md                      # Model usage instructions
│   ├── E7.5_rep1/                     # Models for each dataset
│   ├── E7.5_rep2/
│   └── T_Cells/
│
├── paper_figures/                     # Cross-dataset figure generation scripts
│
├── preprocessing/                     # Dataset-specific preprocessing
│   ├── 00_pbmc_preprocessing_and_tcell_subset.R
│   └── README.md
│
├── docs/                              # Additional documentation
│   ├── DIRECTORY_STRUCTURE.md
│   ├── MULTI_DATASET_GUIDE.md         # Detailed multi-dataset guide
│   └── SLURM_SERIAL_PARALLEL_EXPLAINED.md  # SLURM concepts tutorial
│
├── requirements.txt                   # Python dependencies
├── r_requirements.txt                 # R package list (reference)
├── test_installation.R                # Verify all dependencies are installed
├── CITATION.cff                       # Citation metadata
├── LICENSE                            # MIT License
└── README.md
```



## High-Performance Computing (HPC) Usage

### Recommended: Multi-Dataset Submission (automatic dependency chains)

The preferred way to run on HPC is the multi-dataset system, which automatically manages SLURM job dependencies (serial steps within each dataset, parallel across datasets):

```bash
cd combined/   # or rna_only/ or atac_only/

# 1. Configure cluster settings
cp submit_settings.template.sh submit_settings.sh
nano submit_settings.sh    # Set partition, conda env, resources per step

# 2. Create per-dataset configs in datasets/
#    For combined:     cp R/config_template.R datasets/my_sample.config.R
#    For rna/atac:     cp config_template.R datasets/my_sample.config.R
cp R/config_template.R datasets/my_sample.config.R   # adjust path for rna_only/atac_only
nano datasets/my_sample.config.R

# 3. Submit
./submit_datasets.sh --dry-run    # Preview the job plan
./submit_datasets.sh              # Submit all datasets
```

See [Running Multiple Datasets](#running-multiple-datasets) for full details.

## Target Gene Configuration

### Overview

Target genes define **which genes' expression levels to predict** (the response variable Y). Two types are supported:

- **HVG (Highly Variable Genes)**: Top genes with high cell-to-cell expression variability
- **Random genes (Non-HVG)**: Control set of non-HVG genes meeting minimum expression thresholds

The pipeline uses **pre-computed target gene lists**. The same gene lists must be used across all three pipelines (combined, rna_only, atac_only) for fair comparison.

### Using Pre-computed Target Genes (Default)

> ⚠️ **IMPORTANT**: To reproduce published results, you **MUST** use the pre-computed target gene files.

```r
# In config.R — set paths to the pre-computed gene list files
# Use ABSOLUTE paths or paths relative to your working directory
HVG_GENE_FILE <- "data/target_genes/E7.5_rep2/target_genes_hvg_100.txt"
RANDOM_GENE_FILE <- "data/target_genes/E7.5_rep2/target_genes_random_100.txt"
```

Available pre-computed files:
| Dataset | Path |
|---------|------|
| E7.5_rep1 | `data/target_genes/E7.5_rep1/target_genes_*.txt` |
| E7.5_rep2 | `data/target_genes/E7.5_rep2/target_genes_*.txt` |
| T_Cells | `data/target_genes/T_Cells/target_genes_*.txt` |

### Auto-selection for New Datasets (Optional)

For analyzing **new datasets** without pre-computed gene lists, you can run the standalone target gene selection script:

```bash
# 1. Uncomment the auto-selection parameters in your config.R
# 2. Run the standalone script (NOT part of the default pipeline):
cd combined
Rscript R/02b_select_target_genes.R
```

This generates gene lists that you then reference in all three pipeline configs. See `02b_select_target_genes.R` header comments for details.

## Reproducing Published Results

### Generating Cross-Dataset Figures

After completing analysis on all datasets and models:

```bash
cd paper_figures
Rscript Multi_Dataset_Combined_Performance_Plots_HVG_SET.R
```

## Data Availability

| Dataset | Description | Species | GEO Accession |
|---------|-------------|---------|---------------|
| E7.5_rep1 | Mouse embryo E7.5 (replicate 1) | *Mus musculus* | `GSE205117` |
| E7.5_rep2 | Mouse embryo E7.5 (replicate 2) | *Mus musculus* | `GSE205117` |
| T_Cells | PBMC T cells (subset from 10x PBMC 10k) | *Homo sapiens* | [10x Genomics](https://www.10xgenomics.com/datasets/10-k-human-pbm-cs-multiome-v-1-0-chromium-x-1-standard-2-0-0) |

> **Note**: The T_Cells dataset was derived from the 10x Genomics PBMC 10k multiome dataset. T cells were identified using SingleR cell type annotation and extracted as a subset. See `preprocessing/` for the extraction script. The output goes directly to Step 02 (Data Splitting), skipping Step 01 (QC).

## Citation

If you use scMultiPreDICT in your research, please cite:

```bibtex
[Citation to be added upon publication]
```

## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

## Troubleshooting

### Common Issues

#### 1. "config.R not found"

```bash
# Make sure you're in the pipeline directory and have created config.R
cd combined/
cp R/config_template.R config.R
nano config.R  # Edit with your paths

# Then run the pipeline
Rscript run_pipeline.R --config config.R
```

#### 2. Missing fragments.tsv.gz.tbi (tabix index)

If your ATAC fragments file doesn't have an index:

```bash
# Ensure fragments file is sorted and bgzip compressed
# If not, sort and compress:
sort -k1,1 -k2,2n fragments.tsv | bgzip > fragments.tsv.gz

# Create tabix index
tabix -p bed fragments.tsv.gz
```

#### 3. Out of Memory
```r
# Use disk-backed matrices in config.R
USE_DISK_BACKED <- TRUE

# Or increase memory allocation in submit_settings.sh
STEP_5_MEM="512G"
```

#### 4. "Gene not found in expression matrix"
```r
# Check gene names match your annotation
# Mouse genes: Nanog, Sox2, Pou5f1
# Human genes: NANOG, SOX2, POU5F1
```

#### 5. TensorFlow/Keras Not Found
```bash
# Ensure correct conda environment
conda activate r-bioc-43

# Check TensorFlow installation
python -c "import tensorflow; print(tensorflow.__version__)"
```

#### 6. R Library Path Issues (HPC clusters)

If you encounter package conflicts on HPC systems:

```bash
# Set R library paths explicitly for your conda environment
source ~/miniconda3/etc/profile.d/conda.sh
conda activate r-bioc-43
export R_LIBS="$CONDA_PREFIX/lib/R/library"
export R_LIBS_USER="$CONDA_PREFIX/lib/R/library"
export R_LIBS_SITE=""

# Then run the pipeline
Rscript run_pipeline.R --config config.R
```

## Contact

For questions, bug reports, or feature requests, please open an issue on the [GitHub repository](https://github.com/UzunLab/scMultiPreDICT/issues).
